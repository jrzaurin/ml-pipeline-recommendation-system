{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "PROCESSED_DATA_DIR = Path(\"../data/processed/amazon\")\n",
    "MAX_VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_descriptions = pd.read_pickle(\n",
    "    PROCESSED_DATA_DIR / \"tokenized_descriptions.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>description</th>\n",
       "      <th>description_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000143588</td>\n",
       "      <td>[barefoot, contessa, volume, this, three, disc...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000143561</td>\n",
       "      <td>[giada, de, laurentis, everyday, italian, dvds...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001499572</td>\n",
       "      <td>[like, new]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001526863</td>\n",
       "      <td>[steve, green, hide, -pron-, your, heart, 13, ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001421409</td>\n",
       "      <td>[angel, show, dumitru, california, las, vegas,...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item                                        description  \\\n",
       "0  0000143588  [barefoot, contessa, volume, this, three, disc...   \n",
       "1  0000143561  [giada, de, laurentis, everyday, italian, dvds...   \n",
       "2  0001499572                                        [like, new]   \n",
       "3  0001526863  [steve, green, hide, -pron-, your, heart, 13, ...   \n",
       "4  0001421409  [angel, show, dumitru, california, las, vegas,...   \n",
       "\n",
       "   description_length  \n",
       "0                  67  \n",
       "1                  49  \n",
       "2                   2  \n",
       "3                  14  \n",
       "4                 128  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_descriptions.sample(int(0.8 * tokenized_descriptions.shape[0]))\n",
    "test = tokenized_descriptions.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_features=MAX_VOCAB_SIZE, stop_words=STOP_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_desc = [\" \".join(d) for d in train.description.tolist()]\n",
    "te_desc = [\" \".join(d) for d in test.description.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/amazon_eda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "X_tr_sk = vectorizer.fit_transform(tr_desc)\n",
    "X_te_sk = vectorizer.transform(te_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict()\n",
    "for k, v in vectorizer.vocabulary_.items():\n",
    "    id2word[v] = k\n",
    "X_tr_gen = Sparse2Corpus(X_tr_sk, documents_columns=False)\n",
    "X_te_gen = Sparse2Corpus(X_te_sk, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS\n",
    "NB_TOPICS = 10\n",
    "decay = 0.7\n",
    "offset = 1.\n",
    "max_iterations = 10\n",
    "batch_size = 200\n",
    "max_e_steps = 100\n",
    "eval_every = 1\n",
    "mode = \"online\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SKLEARN\n",
    "lda_sklearn = LatentDirichletAllocation(\n",
    "    n_components=NB_TOPICS,\n",
    "    batch_size=batch_size,\n",
    "    learning_decay=decay,\n",
    "    learning_offset=offset,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    max_iter=max_iterations,\n",
    "    learning_method=mode,\n",
    "    max_doc_update_iter=max_e_steps,\n",
    "    evaluate_every=eval_every)\n",
    "\n",
    "start = time()\n",
    "lda_sklearn.fit(X_tr_sk)\n",
    "sk_time = time() - start\n",
    "\n",
    "sklearn_perplexity = lda_sklearn.perplexity(X_te_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENSIM\n",
    "start = time()\n",
    "lda_gensim_mc = LdaMulticore(\n",
    "    X_tr_gen,\n",
    "    id2word=id2word,\n",
    "    decay=decay,\n",
    "    offset=offset,\n",
    "    num_topics=NB_TOPICS,\n",
    "    passes=max_iterations,\n",
    "    batch=False, #for online training\n",
    "    chunksize=batch_size,\n",
    "    iterations=max_e_steps,\n",
    "    eval_every=eval_every)\n",
    "gn_time = time() - start\n",
    "\n",
    "log_prep_gensim_mc   = lda_gensim_mc.log_perplexity(X_te_gen)\n",
    "preplexity_gensim_mc = np.exp(-1.*log_prep_gensim_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim run time and perplexity: 348.2337691783905, 2617.3677838127815\n",
      "sklearn run time and perplexity: 235.26577377319336, 2761.7179135533775\n"
     ]
    }
   ],
   "source": [
    "print(\"gensim run time and perplexity: {}, {}\".format(gn_time, preplexity_gensim_mc))\n",
    "print(\"sklearn run time and perplexity: {}, {}\".format(sk_time, sklearn_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn \n",
      "\n",
      "    topic_0     topic_1         topic_2 topic_3 topic_4  topic_5 topic_6  \\\n",
      "0      film      strong           false    life   class       br     man   \n",
      "1     story          li          locked    love  images  episode    find   \n",
      "2     world         dvd        priority  dempty  amazon   season   woman   \n",
      "3       war        disc              de  family    href   series    turn   \n",
      "4  director          vs      semihidden    find     com     star    come   \n",
      "5  american  collection  unhidewhenused    live   https   comedy  murder   \n",
      "6       new   christmas    lsdexception   world     div       tv   young   \n",
      "7   history     feature              la    year      na  include   movie   \n",
      "8        em         new          accent   young     ssl     love    star   \n",
      "9   include       video            true   child    spin     john    girl   \n",
      "\n",
      "   topic_7   topic_8     topic_9  \n",
      "0    music    amazon         dvd  \n",
      "1     live       com      player  \n",
      "2  include       dvd     english  \n",
      "3     song       use      region  \n",
      "4      dvd    medium         pal  \n",
      "5    great   product        play  \n",
      "6    world     apply    features  \n",
      "7     year  standard     digital  \n",
      "8   minute    demand         ray  \n",
      "9     band      body  dvd_player  \n",
      "\n",
      "\n",
      "Gensim \n",
      "\n",
      "          topic_0  topic_1     topic_2  topic_3 topic_4    topic_5 topic_6  \\\n",
      "0              br   amazon         dvd     film    life    episode  strong   \n",
      "1           false      com      player    world    find     season    star   \n",
      "2          locked      dvd      region  include    love     series    film   \n",
      "3        priority  product        play    music   young        new      em   \n",
      "4      semihidden      use        film  feature     man  adventure   movie   \n",
      "5  unhidewhenused   return         new     year   story       time     man   \n",
      "6    lsdexception   demand  dvd_player     time    year      world      de   \n",
      "7            true   medium     english    story  family    include    play   \n",
      "8          accent    apply         pal      new   woman     friend    love   \n",
      "9             vhs   policy          vs  history    live       find    john   \n",
      "\n",
      "  topic_7   topic_8  topic_9  \n",
      "0  images     class   dempty  \n",
      "1  amazon        li      dvd  \n",
      "2      na       div     body  \n",
      "3     ssl      href  workout  \n",
      "4     jpg    amazon    dance  \n",
      "5     com      spin    learn  \n",
      "6   https        ie    video  \n",
      "7      td       com      use  \n",
      "8  border    images   minute  \n",
      "9    href  carousel     yoga  \n"
     ]
    }
   ],
   "source": [
    "topic_words = dict()\n",
    "gensim_topics = lda_gensim_mc.show_topics(formatted=False)\n",
    "def sklearn_show_topics(model, feature_names, n_top_words):\n",
    "    sk_topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        tot_score = np.sum(topic)\n",
    "        top_words = [(feature_names[i],topic[i]/tot_score)\n",
    "            for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        sk_topics.append([topic_idx,top_words])\n",
    "    return sk_topics\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "sklearn_topics = sklearn_show_topics(lda_sklearn, feature_names,10)\n",
    "topic_words['gensim']  = gensim_topics\n",
    "topic_words['sklearn'] = sklearn_topics\n",
    "\n",
    "# or in data frame formta\n",
    "topic_words_df = dict()\n",
    "for model, result in topic_words.items():\n",
    "    df = pd.DataFrame()\n",
    "    for topic in result:\n",
    "        cols =  [[word[0] for word in topic[1]] for topic in result]\n",
    "        for i,c in enumerate(cols):\n",
    "            df[\"topic_\"+str(i)] = c\n",
    "    topic_words_df[model] = df\n",
    "\n",
    "print('Sklearn \\n')\n",
    "print(topic_words_df['sklearn'])\n",
    "print('\\n')\n",
    "print('Gensim \\n')\n",
    "print(topic_words_df['gensim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_descriptions.sample(5000)\n",
    "test = tokenized_descriptions.drop(train.index).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_features=MAX_VOCAB_SIZE, stop_words=STOP_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_desc = [\" \".join(d) for d in train.description.tolist()]\n",
    "te_desc = [\" \".join(d) for d in test.description.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/amazon_eda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "X_tr_sk = vectorizer.fit_transform(tr_desc)\n",
    "X_te_sk = vectorizer.transform(te_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict()\n",
    "for k, v in vectorizer.vocabulary_.items():\n",
    "    id2word[v] = k\n",
    "X_tr_gen = Sparse2Corpus(X_tr_sk, documents_columns=False)\n",
    "X_te_gen = Sparse2Corpus(X_te_sk, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS\n",
    "decay = 0.7\n",
    "offset = 1.\n",
    "max_iterations = 10\n",
    "batch_size = 200\n",
    "max_e_steps = 100\n",
    "eval_every = 1\n",
    "mode = \"online\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6322.687323963289\n",
      "10 8164.051691175491\n",
      "20 10627.883780705944\n",
      "30 12783.048442798514\n"
     ]
    }
   ],
   "source": [
    "for n_topics in [5, 10, 20, 30]:\n",
    "    lda_sklearn = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        batch_size=batch_size,\n",
    "        learning_decay=decay,\n",
    "        learning_offset=offset,\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "        max_iter=max_iterations,\n",
    "        learning_method=mode,\n",
    "        max_doc_update_iter=max_e_steps,\n",
    "        evaluate_every=eval_every)\n",
    "    lda_sklearn.fit(X_tr_sk)  \n",
    "    sklearn_perplexity = lda_sklearn.perplexity(X_te_sk)   \n",
    "    print(n_topics, sklearn_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 7797.699262434815\n",
      "10 15881.674141667338\n",
      "20 61983.10738771032\n",
      "30 192539.38416285504\n"
     ]
    }
   ],
   "source": [
    "for n_topics in [5, 10, 20, 30]:\n",
    "    lda_gensim_mc = LdaMulticore(\n",
    "        X_tr_gen,\n",
    "        id2word=id2word,\n",
    "        decay=decay,\n",
    "        offset=offset,\n",
    "        num_topics=n_topics,\n",
    "        passes=max_iterations,\n",
    "        batch=False, #for online training\n",
    "        chunksize=batch_size,\n",
    "        iterations=max_e_steps,\n",
    "        eval_every=eval_every)\n",
    "    log_prep_gensim_mc   = lda_gensim_mc.log_perplexity(X_te_gen)\n",
    "    preplexity_gensim_mc = np.exp(-1.*log_prep_gensim_mc)\n",
    "    print(n_topics, preplexity_gensim_mc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazon_eda)",
   "language": "python",
   "name": "conda_amazon_eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
